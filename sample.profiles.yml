
# This configuration file specifies information about connections to
# your data warehouse(s). The file contains a series of "profiles."
# Profiles specify database credentials and connection information
#
# By default, dbt looks for this file in ~/.dbt/profiles.yml. That option
# can be configured when dbt is invoked with the --profiles-dir option:
#
#  $ dbt run --profiles-dir /opt/dbt/

# Top-level configs that apply to all profiles are set here
config:
    send_anonymous_usage_stats: True
    use_colors: True

# Profiles configurations should adhere to the structure defined below:
#
# Postgres / Redshift
# -------------------
#
#   [profile-name]:
#     outputs:
#       [target-name]:
#         type: {redshift, postgres}
#         threads: [1 - 8]
#         host: [host ip or fully qualified domain name]
#         port: [port]
#         user: [user]
#         pass: [password]
#         dbname: [dbname]
#         schema: [schema name]
#     target: [target-name]
#
#
# Snowflake
# -------------------
#
#   [profile-name]:
#     outputs:
#       [target-name]:
#         type: snowflake
#         threads: [1 - 8]
#         account: [url prefix for your snowflake connection]
#
#         user: [user]
#         password: [password]
#         role: [optional, the snowflake role you want to use]
#
#         database: [db name]
#         warehouse: [warehouse]
#         schema: [schema name]
#     target: [target-name]
#
#
# BigQuery
# -------------------
#   [profile-name]:
#     target: [target-name]
#
#     outputs:
#       # 1. Use oauth flow
#       [target-name-1]:
#         type: bigquery
#         method: oauth
#         project: [GCP project id]
#         schema: [dbt schema]
#         threads: [between 1 and 8]
#         timeout_seconds: 300
#
#       # 2. use a service account keyfile
#       [target-name-2]:
#         type: bigquery
#         method: service-account
#         project: [GCP project id]
#         schema: [dbt schema]
#         threads: [between 1 and 8]
#         keyfile: [/path/to/bigquery/keyfile.json]
#
#       # 3. Use the contents of the service account keyfile (advanced)
#       [target-name-3]:
#         type: bigquery
#         method: service-account-json
#         project: [GCP project id]
#         schema: [dbt schema]
#         threads: [between 1 and 8]
#
#         # These fields come from the service account json keyfile
#         keyfile_json:
#           type: xxx
#           project_id: xxx
#           private_key_id: xxx
#           private_key: xxx
#           client_email: xxx
#           client_id: xxx
#           auth_uri: xxx
#           token_uri: xxx
#           auth_provider_x509_cert_url: xxx
#           client_x509_cert_url: xxx
#
#
# Commonly, it's helpful to define multiple targets for a profile. For example,
# these targets might be `dev` and `prod`. Whereas the `dev` target points to
# a development schema (eg. dbt_dev), the `prod` schema should point to the
# prod schema (eg. analytics). Analytical/BI tools should point to the
# prod schema so that local development does not interfere with analysis.
#
# The following are some examples of well-formatted profile configurations

evil-corp:
    outputs:
        dev:                      # specify the dev connection
            type: redshift
            threads: 1
            host: 12.34.56.78
            port: 5439
            user: elliot
            pass: pa55word
            dbname: warehouse
            schema: dbt_elliot    # use the dev schema
        prod:                     # specify the prod connection
            type: redshift
            threads: 1
            host: 12.34.56.78
            port: 5439
            user: elliot
            pass: pa55word
            dbname: warehouse
            schema: analytics     # use the prod schema instead
        snowflake:                # specify the snowflake connection
            type: snowflake
            threads: 1
            account: evilcorp     # the url prefix for your snowflake connection,
                                  # i.e. evilcorp.snowflakecomputing.com
            user: elliot
            password: pa55word
            role: SYSADMIN        # optional, the snowflake role you want to use
                                  # when connecting
            database: db
            warehouse: warehouse
            schema: analytics     # use the prod schema instead

        bigquery:
            type: bigquery
            method: oauth
            project: evil-corp
            schema: analytics
            threads: 1

    target: dev                   # default target is dev unless changed at run time

mr-robot:
    outputs:
        dev:                      # specify the dev connection
            type: postgres
            threads: 2
            host: 87.65.43.21
            port: 5439
            user: mr_robot
            pass: password1
            dbname: warehouse
            schema: dbt_mr_robot  # use the dev schema
        prod:                     # specify the prod connection
            type: postgres
            threads: 1
            host: 87.65.43.21
            port: 5439
            user: mr_robot
            pass: password1
            dbname: warehouse
            schema: analytics     # use the prod schema instead
    target: dev                   # default target is dev unless changed at run time

# You can switch between profiles and targets on the command line. All of the
# following are valid ways to invoke dbt run/test/compile/etc
#
# $ dbt run --profile evil-corp
# $ dbt run --profile evil-corp --target dev
# $ dbt run --profile evil-corp --target prod
#
# $ dbt run --profile mr-robot
# $ dbt run --profile mr-robot --target dev
# $ dbt run --profile mr-robot --target prod
